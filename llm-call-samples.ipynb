{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "098d535f-3c75-4267-8cde-27f8df66308a",
   "metadata": {},
   "source": [
    "# Introduction to Databricks LLM\n",
    "\n",
    "This notebook shows how to query Databricks LLM endpoints. I will show you several different approaches:\n",
    "- databricks.sdk.WorkspaceClient.serving_endpoints.query (used by Databricks examples in Streamlit)\n",
    "- openai compatible API (for streaming and tools)\n",
    "- langchain (many other Databricks examples)\n",
    "\n",
    "Out of the scope:\n",
    "- MLFLOW: https://mlflow.org/docs/latest/api_reference/python_api/mlflow.deployments.html#mlflow.deployments.DatabricksDeploymentClient \n",
    "- REST API\n",
    "- reasoning (claude sonnet)\n",
    "- image recognition (claude sonnet)\n",
    "- full list: https://learn.microsoft.com/en-us/azure/databricks/machine-learning/model-serving/score-foundation-models\n",
    "\n",
    "Besides that in this notebook I show you how to call functions, including the tools created in Unity Catalog as UDF.\n",
    "\n",
    "Maintainer: pavel_goncharenko@epam.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ca709e",
   "metadata": {},
   "source": [
    "## How to get the Databricks:\n",
    "At least two free options are available for you:\n",
    "- You can create Databricks Workspace using MSDN subscription in Azure: https://learn.microsoft.com/en-us/azure/databricks/getting-started/free-trial\n",
    "- You can request an EPAM's Databricks Sandbox here: https://kb.epam.com/display/EPMCBDCC/Databricks+CoE#DatabricksCoE-DATABRICKSSANDBOX\n",
    "\n",
    "## Authorization\n",
    "You have several options, I will show two of them:\n",
    "- Create a service principal (you need admin privileges). Generate a key-pair (`DATABRICKS_CLIENT_ID` and `DATABRICKS_CLIENT_ID`) and use it below.\n",
    "- You can create your user's PAT in Settings/Developer menu of Databricks Workspace. Place your PAT to `DATABRICKS_TOKEN` below.\n",
    "\n",
    "## Requred permissions:\n",
    "- Permissions on the workspace: User\n",
    "- Permissions on the SQL warehouse: Can use\n",
    "- Permissions on the `sample` catalog (by default there)\n",
    "\n",
    "## Please add your credentials below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c7c009",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13bfcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workspace url: https://adb-327....azuredatabricks.net/\n",
    "os.environ[\"DATABRICKS_HOST\"] = ...\n",
    "# a) EITHER Service principal oauth credentials:\n",
    "os.environ[\"DATABRICKS_CLIENT_ID\"] = ...\n",
    "os.environ[\"DATABRICKS_CLIENT_SECRET\"] = ...\n",
    "# b) OR user's personal access token:\n",
    "os.environ[\"DATABRICKS_TOKEN\"] = ...\n",
    "# SQL warehouse id for SQL queries:\n",
    "os.environ[\"DATABRICKS_WAREHOUSE_ID\"] = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78238911",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5546f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing conflicting packages from Colab\n",
    "%pip uninstall -y pyspark pyspark-connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "263ae331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Dependencies for the notebook:\n",
    "%pip install -qq python-dotenv\n",
    "%pip install -qq databricks-sdk[openai]==0.50.0\n",
    "%pip install -qq databricks-langchain==0.4.2\n",
    "%pip install -qq databricks-sql-connector==4.0.3\n",
    "%pip install -qq databricks-connect==16.1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a167a74e",
   "metadata": {},
   "source": [
    "# RESTART Colab session here once:\n",
    "Runtime > Restart session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aded8771-154c-4d23-82e3-e43301204380",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd # IF FAILS HERE JUST RESTART Colab: \"Runtime > Restart session\" and run again.\n",
    "from pprint import pprint\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "from databricks import sql\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.serving import ChatMessage, ChatMessageRole\n",
    "from databricks.sdk.core import Config\n",
    "from databricks_langchain import ChatDatabricks, UCFunctionToolkit\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, ToolMessage, BaseMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.tools import tool\n",
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from unitycatalog.ai.core.databricks import DatabricksFunctionClient\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def display_json(obj):\n",
    "    \"\"\"Display a JSON object in a formatted way.\"\"\"\n",
    "    json_str = json.dumps(obj, indent=2)\n",
    "    display(Markdown(\"```json\\n\" + json_str + \"\\n```\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f729aa74-f7df-409b-ade8-ecbecbc59c5c",
   "metadata": {},
   "source": [
    "## WorkspaceClient instance\n",
    "Instantiate a WorkspaceClient instance. It requred to make LLM calls to Databricks API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e86ab2dc-c043-499a-81b1-646da546f49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from databricks.sdk import WorkspaceClient\n",
    "\n",
    "client = WorkspaceClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b111f967-fbb9-49a8-8c28-dde4b6f765fe",
   "metadata": {},
   "source": [
    "## Sample messages we will use:\n",
    "We will use the message asking how many rows we have in sample table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2ef5ff2-d0cd-478c-bb1f-e6a2cc499fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are helpful assistant, please provide only reliable information based on the context.\"},\n",
    "    {\"role\": \"user\", \"content\": \"How many records in my table samples.tpch.customer?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf23fb34-8c5d-47c9-b740-e5de96062316",
   "metadata": {},
   "source": [
    "## We will query against this endpoint\n",
    "\n",
    "The list of available endpoins in Databricks: Machine Learning -> Serving\n",
    "\n",
    "Doc: https://learn.microsoft.com/en-us/azure/databricks/machine-learning/model-serving/foundation-model-overview\n",
    "\n",
    "This endpoint is pay-per-token and already deployed and available to use by default. You can use any other Chat model to play with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0184842a-6cc3-4879-8c1f-c3bca5bdd285",
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVING_ENDPOINT = \"databricks-meta-llama-3-3-70b-instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f845351-5370-46f0-bdc3-82cf142dd840",
   "metadata": {},
   "source": [
    "# 1. Databricks `serving_endpoints` API\n",
    "\n",
    "- serving_endpoints.query doesn't work with stream=True\n",
    "- serving_endpoints.query doesn't support tools\n",
    "\n",
    "**Note**:\n",
    "In the response below LLM cannot get the real amount of records, because it doesn't have context or tool calling logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84ec6822-665b-44c0-bb1e-776ee14ada67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ChatMessage(content='You are helpful assistant, please provide only reliable '\n",
      "                     'information based on the context.',\n",
      "             role=<ChatMessageRole.SYSTEM: 'system'>),\n",
      " ChatMessage(content='How many records in my table samples.tpch.customer?',\n",
      "             role=<ChatMessageRole.USER: 'user'>)]\n"
     ]
    }
   ],
   "source": [
    "# from databricks.sdk.service.serving import ChatMessage, ChatMessageRole\n",
    "\n",
    "# Converting dict to ChatMessage:\n",
    "dbx_messages = [\n",
    "    ChatMessage(role=ChatMessageRole(msg[\"role\"]), content=msg[\"content\"])\n",
    "    for msg in messages\n",
    "]\n",
    "\n",
    "pprint(dbx_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2689d20e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## LLM raw output:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QueryEndpointResponse(choices=[V1ResponseChoiceElement(finish_reason=None,\n",
      "                                                       index=0,\n",
      "                                                       logprobs=None,\n",
      "                                                       message=ChatMessage(content=\"I'm a large language model, I don't have direct access to your database or tables. However, I can guide you on how \"\n",
      "                                                                                   'to find the number of records in your table.\\n'\n",
      "                                                                                   '\\n'\n",
      "                                                                                   'To get the number of records in your table `samples.tpch.customer`, you can use a SQL query:\\n'\n",
      "                                                                                   '\\n'\n",
      "                                                                                   '```sql\\n'\n",
      "                                                                                   'SELECT COUNT(*) FROM samples.tpch.customer;\\n'\n",
      "                                                                                   '```\\n'\n",
      "                                                                                   '\\n'\n",
      "                                                                                   'This query will return the number of rows in your table. Please execute this query in your database management '\n",
      "                                                                                   'system or SQL client to get the exact count.',\n",
      "                                                                           role=<ChatMessageRole.ASSISTANT: 'assistant'>),\n",
      "                                                       text=None)],\n",
      "                      created=1745508392,\n",
      "                      data=[],\n",
      "                      id='chatcmpl_0079c8a2-3f42-42e3-ad21-5c64bf628d8e',\n",
      "                      model='meta-llama-3.3-70b-instruct-121024',\n",
      "                      object=<QueryEndpointResponseObject.CHAT_COMPLETION: 'chat.completion'>,\n",
      "                      predictions=None,\n",
      "                      served_model_name=None,\n",
      "                      usage=ExternalModelUsageElement(completion_tokens=104, prompt_tokens=41, total_tokens=145))\n"
     ]
    }
   ],
   "source": [
    "chat_completion = client.serving_endpoints.query(\n",
    "    name=SERVING_ENDPOINT,\n",
    "    messages=dbx_messages,\n",
    "    max_tokens=500,\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "display(Markdown(\"## LLM raw output:\"))\n",
    "pprint(chat_completion, width=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fb9d3bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## LLM response:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "I'm a large language model, I don't have direct access to your database or tables. However, I can guide you on how to find the number of records in your table.\n",
       "\n",
       "To get the number of records in your table `samples.tpch.customer`, you can use a SQL query:\n",
       "\n",
       "```sql\n",
       "SELECT COUNT(*) FROM samples.tpch.customer;\n",
       "```\n",
       "\n",
       "This query will return the number of rows in your table. Please execute this query in your database management system or SQL client to get the exact count."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(\"## LLM response:\"))\n",
    "display(Markdown(chat_completion.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c90a7a-b21c-436f-a47f-4554f1dee5df",
   "metadata": {},
   "source": [
    "# 2. OpenAI compatible API: `get_open_ai_client`\n",
    "\n",
    "## 2.1. Chat Completion with OpenAI client\n",
    "\n",
    "Previous example has limitations. It cannot properly process streams and tools. OpenAI API is better for our goals. Let's use it instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486c612d-9318-42cd-861d-9de8bc8416b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = client.serving_endpoints.get_open_ai_client()\n",
    "\n",
    "chat_completion = openai_client.chat.completions.create(\n",
    "    model=SERVING_ENDPOINT,\n",
    "    messages=messages,\n",
    "    max_tokens=500,\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "display(Markdown(\"### LLM raw output:\"))\n",
    "display_json(chat_completion.model_dump())\n",
    "\n",
    "# **Note**: In the response below LLM cannot get the real amount of records, because it doesn't have context or tool calling logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df675a61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### LLM response Markdown fomatted:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "I'm a large language model, I don't have direct access to your database or tables. However, I can guide you on how to find the number of records in your table.\n",
       "\n",
       "To get the number of records in your table `samples.tpch.customer`, you can use a SQL query:\n",
       "\n",
       "```sql\n",
       "SELECT COUNT(*) FROM samples.tpch.customer;\n",
       "```\n",
       "\n",
       "This query will return the number of rows in your table. Please execute this query in your database management system or SQL client to get the exact count."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(\"### LLM response Markdown fomatted:\"))\n",
    "display(Markdown(chat_completion.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13197e49-844f-45d4-a591-fdfab0d7042e",
   "metadata": {},
   "source": [
    "## 2.2. Streaming example (openai client)\n",
    "\n",
    "Here is the example of streaming. Streaming provides better UI experience. Instead of waiting until the LLM provides full answer - user can see the intermediate results immediately token by token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2022b957-3534-43e8-9506-7ddea0df8d1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I'm a large language model, I don't have direct access to your database or tables. However, I can guide you on how to find the number of records in your table.\n",
       "\n",
       "To get the number of records in your table `samples.tpch.customer`, you can use a SQL query:\n",
       "\n",
       "```sql\n",
       "SELECT COUNT(*) FROM samples.tpch.customer;\n",
       "```\n",
       "\n",
       "This query will return the number of rows in your table. If you're using a specific database management system like MySQL, PostgreSQL, or BigQuery, you can run this query in the respective query editor or terminal.\n",
       "\n",
       "If you provide more context or information about your database setup, I can offer more tailored assistance."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "openai_client = client.serving_endpoints.get_open_ai_client()\n",
    "\n",
    "chat_completion = openai_client.chat.completions.create(\n",
    "    model=SERVING_ENDPOINT,\n",
    "    messages=messages,\n",
    "    max_tokens=500,\n",
    "    temperature=0.1,\n",
    "    stream=True, # Enable streaming\n",
    ")\n",
    "\n",
    "content = \"\"\n",
    "\n",
    "display(Markdown(\"### LLM streaming response:\"))\n",
    "\n",
    "for chunk in chat_completion:\n",
    "    if hasattr(chunk, \"choices\"):\n",
    "        if chunk.choices[0].delta.content:\n",
    "            content += chunk.choices[0].delta.content\n",
    "            clear_output(wait=True)\n",
    "            display(Markdown(content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8157556c-0c11-4b83-bc02-124ca1ea9d8e",
   "metadata": {},
   "source": [
    "# 3. Langchain\n",
    "\n",
    "Doc: https://python.langchain.com/docs/integrations/chat/databricks/\n",
    "\n",
    "Many Databricks examples based on langchain syntax. These examples are for you to provide the basic knowledge and understanding. \n",
    "\n",
    "## 3.1. Langchain invoke\n",
    "\n",
    "See https://python.langchain.com/api_reference/community/chat_models/langchain_community.chat_models.databricks.ChatDatabricks.html for other supported parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fff6acc-29c3-4261-bdfb-38dc079b470f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### LLM raw output:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```json\n",
       "{\n",
       "  \"content\": \"I'm a large language model, I don't have direct access to your database or tables. However, I can guide you on how to find the number of records in your table.\\n\\nTo get the number of records in your table `samples.tpch.customer`, you can use a SQL query:\\n\\n```sql\\nSELECT COUNT(*) FROM samples.tpch.customer;\\n```\\n\\nThis query will return the number of rows in your table. If you're using a specific database management system like MySQL, PostgreSQL, or BigQuery, you can run this query in the respective query editor or terminal.\\n\\nIf you provide more context or information about your database setup, I can offer more tailored guidance.\",\n",
       "  \"additional_kwargs\": {},\n",
       "  \"response_metadata\": {\n",
       "    \"id\": \"chatcmpl_07f2ce54-cd86-4b96-bf5e-0cc6db792fb9\",\n",
       "    \"object\": \"chat.completion\",\n",
       "    \"created\": 1745507463,\n",
       "    \"model\": \"meta-llama-3.3-70b-instruct-121024\",\n",
       "    \"usage\": {\n",
       "      \"prompt_tokens\": 41,\n",
       "      \"completion_tokens\": 136,\n",
       "      \"total_tokens\": 177\n",
       "    },\n",
       "    \"model_name\": \"meta-llama-3.3-70b-instruct-121024\"\n",
       "  },\n",
       "  \"type\": \"ai\",\n",
       "  \"name\": null,\n",
       "  \"id\": \"run-54dc8a85-adf4-4b80-aebf-a270d51c1728-0\",\n",
       "  \"example\": false,\n",
       "  \"tool_calls\": [],\n",
       "  \"invalid_tool_calls\": [],\n",
       "  \"usage_metadata\": null\n",
       "}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from databricks_langchain import ChatDatabricks\n",
    "\n",
    "# If you use .databrickscfg and not a `default` profile, override the `target_uri` like this:\n",
    "# class MyChatDatabricks(ChatDatabricks):\n",
    "#     target_uri: str = \"databricks://<your_profile_name>\"\n",
    "\n",
    "langchain_chat_model = ChatDatabricks(\n",
    "    endpoint=SERVING_ENDPOINT,\n",
    "    temperature=0.1,\n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "result = langchain_chat_model.invoke(messages)\n",
    "\n",
    "display(Markdown(\"### LLM raw output:\"))\n",
    "display_json(vars(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae20b8b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### LLM response Markdown formatted:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "I'm a large language model, I don't have direct access to your database or tables. However, I can guide you on how to find the number of records in your table.\n",
       "\n",
       "To get the number of records in your table `samples.tpch.customer`, you can use a SQL query:\n",
       "\n",
       "```sql\n",
       "SELECT COUNT(*) FROM samples.tpch.customer;\n",
       "```\n",
       "\n",
       "This query will return the number of rows in your table. If you're using a specific database management system like MySQL, PostgreSQL, or BigQuery, you can run this query in the respective query editor or terminal.\n",
       "\n",
       "If you provide more context or information about your database setup, I can offer more tailored guidance."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(\"### LLM response Markdown formatted:\"))\n",
    "display(Markdown(result.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbd4495-5bf3-4f88-bafd-608e85f529a0",
   "metadata": {},
   "source": [
    "## 3.2. Langchain Stream\n",
    "\n",
    "Stream is also supported in Databricks langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7cc1f6-c190-4afd-be51-7dca1ae4441c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### LLM streaming response Markdown formatted:\n",
       "I'm a large language model, I don't have direct access to your database or tables. However, I can guide you on how to find the number of records in your table.\n",
       "\n",
       "To get the number of records in your table `samples.tpch.customer`, you can use a SQL query:\n",
       "\n",
       "```sql\n",
       "SELECT COUNT(*) FROM samples.tpch.customer;\n",
       "```\n",
       "\n",
       "This query will return the number of rows in your table. Please execute this query in your database management system to get the exact count."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "langchain_chat_model = ChatDatabricks(\n",
    "    endpoint=SERVING_ENDPOINT,\n",
    "    temperature=0.1,\n",
    "    max_tokens=500,\n",
    ")\n",
    "\n",
    "result = langchain_chat_model.stream(messages)\n",
    "\n",
    "content = \"### LLM streaming response Markdown formatted:\\n\"\n",
    "\n",
    "for chunk in result:\n",
    "    content += chunk.content\n",
    "    clear_output(wait=True)\n",
    "    display(Markdown(content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce63344b-e479-4e67-89cd-ae0d1aed0828",
   "metadata": {},
   "source": [
    "# 4. Function calling\n",
    "\n",
    "Function calling supported by many LLMs, please check the documentation of your LLM model. The documentation about function calling here: https://platform.openai.com/docs/assistants/tools/function-calling\n",
    "\n",
    "The basic idea is:\n",
    "\n",
    "- developer says to LLM: \"I have a function `sql_query`, you can execute it with parameter `query`, it executes any SQL query against my database, call it if required\".\n",
    "- user asks LLM: \"How many rows in table `sample`?\"\n",
    "- LLM responds to system: \"Please execute function `sql_query` with `query`='select count(*) from sample'\"\n",
    "- system designed to call this function and provide the result back to LLM: `200`\n",
    "- LLM replies to user: \"You have 200 records in table `sample`\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d003b27-ac71-4196-bfc1-1cdd5aa61a06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### LLM raw output:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```json\n",
       "{\n",
       "  \"id\": \"chatcmpl_a84847a7-09e7-424f-80c9-2116b7c76674\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"tool_calls\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"message\": {\n",
       "        \"content\": null,\n",
       "        \"refusal\": null,\n",
       "        \"role\": \"assistant\",\n",
       "        \"annotations\": null,\n",
       "        \"audio\": null,\n",
       "        \"function_call\": null,\n",
       "        \"tool_calls\": [\n",
       "          {\n",
       "            \"id\": \"call_ebbae73b-2ec6-46ad-bc7c-b0417ac2e214\",\n",
       "            \"function\": {\n",
       "              \"arguments\": \"{\\\"query\\\": \\\"SELECT COUNT(*) FROM samples.tpch.customer\\\"}\",\n",
       "              \"name\": \"sql_query\"\n",
       "            },\n",
       "            \"type\": \"function\"\n",
       "          }\n",
       "        ]\n",
       "      }\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1745507470,\n",
       "  \"model\": \"meta-llama-3.3-70b-instruct-121024\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"service_tier\": null,\n",
       "  \"system_fingerprint\": null,\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 21,\n",
       "    \"prompt_tokens\": 715,\n",
       "    \"total_tokens\": 736,\n",
       "    \"completion_tokens_details\": null,\n",
       "    \"prompt_tokens_details\": null\n",
       "  }\n",
       "}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's imagine we have a function sql_query, there is a description for LLM call:\n",
    "sql_query_tool_json = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"sql_query\",\n",
    "        \"description\": \"Execute any query against Databricks\",\n",
    "        \"parameters\": {\n",
    "            \"description\": \"Executing given SQL query and returning a table of results\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"description\": \"SQL query to be executed\",\n",
    "                    \"type\": \"string\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"query\"],\n",
    "            \"type\": \"object\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "openai_client = client.serving_endpoints.get_open_ai_client()\n",
    "\n",
    "chat_completion = openai_client.chat.completions.create(\n",
    "    model=SERVING_ENDPOINT,\n",
    "    messages=messages,\n",
    "    max_tokens=500,\n",
    "    temperature=0.1,\n",
    "    tools=[sql_query_tool_json]\n",
    ")\n",
    "\n",
    "display(Markdown(\"### LLM raw output:\"))\n",
    "display_json(chat_completion.model_dump())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e855716-6f82-4b7a-87f4-19ae81b5ba6e",
   "metadata": {},
   "source": [
    "**Output**:\n",
    "\n",
    "As you can see the output of LLM has:\n",
    "- finish_reason='tool_calls'\n",
    "- content=None\n",
    "- function=Function(...)\n",
    "\n",
    "LLM asks us to call a function with name `sql_query` and pass the following parameters to it, according our function description:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e0c295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function(arguments='{\"query\": \"SELECT COUNT(*) FROM samples.tpch.customer\"}', name='sql_query')\n"
     ]
    }
   ],
   "source": [
    "print(chat_completion.choices[0].message.tool_calls[0].function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616e379e-8d3d-43e0-b05e-9ca2e5979397",
   "metadata": {},
   "source": [
    "**Function calling step:**\n",
    "\n",
    "Let's pretend we executed our function `sql_query` with parameters query=\"SELECT COUNT(*) FROM samples.tpch.customer\" and the function returned the following text:\n",
    "\n",
    "| count(*) |\n",
    "| -------- |\n",
    "|      200 |\n",
    "\n",
    "Now we should add these two messages (from assistant and from our function) to the `messages` list and pass them back to the LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0f5f69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### New messages array:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```json\n",
       "[\n",
       "  {\n",
       "    \"role\": \"system\",\n",
       "    \"content\": \"You are helpful assistant, please provide only reliable information based on the context.\"\n",
       "  },\n",
       "  {\n",
       "    \"role\": \"user\",\n",
       "    \"content\": \"How many records in my table samples.tpch.customer?\"\n",
       "  },\n",
       "  {\n",
       "    \"role\": \"assistant\",\n",
       "    \"tool_calls\": [\n",
       "      {\n",
       "        \"type\": \"function\",\n",
       "        \"id\": \"call_ebbae73b-2ec6-46ad-bc7c-b0417ac2e214\",\n",
       "        \"function\": {\n",
       "          \"name\": \"sql_query\",\n",
       "          \"arguments\": \"{\\\"query\\\": \\\"SELECT COUNT(*) FROM samples.tpch.customer\\\"}\"\n",
       "        }\n",
       "      }\n",
       "    ]\n",
       "  },\n",
       "  {\n",
       "    \"role\": \"tool\",\n",
       "    \"tool_call_id\": \"call_ebbae73b-2ec6-46ad-bc7c-b0417ac2e214\",\n",
       "    \"content\": \"200\"\n",
       "  }\n",
       "]\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_messages = messages.copy()\n",
    "\n",
    "# Adding a message from assistant to our messages array, which asks for tool calling:\n",
    "new_messages.append({\n",
    "    \"role\": \"assistant\",\n",
    "    \"tool_calls\": [{\n",
    "        \"type\": \"function\",\n",
    "        \"id\": chat_completion.choices[0].message.tool_calls[0].id,\n",
    "        \"function\": {\n",
    "            \"name\": chat_completion.choices[0].message.tool_calls[0].function.name,\n",
    "            \"arguments\": chat_completion.choices[0].message.tool_calls[0].function.arguments},\n",
    "    }]\n",
    "})\n",
    "# Adding the function result:\n",
    "new_messages.append({\n",
    "    \"role\": \"tool\",\n",
    "    \"tool_call_id\": chat_completion.choices[0].message.tool_calls[0].id,\n",
    "    \"content\": \"200\"\n",
    "})\n",
    "\n",
    "# New messages array looks like:\n",
    "display(Markdown(\"### New messages array:\"))\n",
    "display_json(new_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20afb50c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### LLM raw output:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```json\n",
       "{\n",
       "  \"id\": \"chatcmpl_d6970271-2722-42f0-a08e-2132ff54f123\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"message\": {\n",
       "        \"content\": \"There are 200 records in your table samples.tpch.customer.\",\n",
       "        \"refusal\": null,\n",
       "        \"role\": \"assistant\",\n",
       "        \"annotations\": null,\n",
       "        \"audio\": null,\n",
       "        \"function_call\": null,\n",
       "        \"tool_calls\": null\n",
       "      }\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1745507471,\n",
       "  \"model\": \"meta-llama-3.3-70b-instruct-121024\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"service_tier\": null,\n",
       "  \"system_fingerprint\": null,\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 14,\n",
       "    \"prompt_tokens\": 755,\n",
       "    \"total_tokens\": 769,\n",
       "    \"completion_tokens_details\": null,\n",
       "    \"prompt_tokens_details\": null\n",
       "  }\n",
       "}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat_completion = openai_client.chat.completions.create(\n",
    "    model=SERVING_ENDPOINT,\n",
    "    messages=new_messages,\n",
    "    max_tokens=500,\n",
    "    tools=[sql_query_tool_json]\n",
    ")\n",
    "\n",
    "display(Markdown(\"### LLM raw output:\"))\n",
    "display_json(chat_completion.model_dump())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2362aefb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### LLM output Markdown formatted:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "There are 200 records in your table samples.tpch.customer."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(\"### LLM output Markdown formatted:\"))\n",
    "display(Markdown(chat_completion.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1156ea07",
   "metadata": {},
   "source": [
    "### Real function example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8541223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Tool real output:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```json\n",
       "{\n",
       "  \"0\": {\n",
       "    \"count(1)\": 750000\n",
       "  }\n",
       "}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# SQL query will be performed against this SQL Wartehouse:\n",
    "DATABRICKS_WAREHOUSE_ID = os.getenv(\"DATABRICKS_WAREHOUSE_ID\")\n",
    "\n",
    "def sql_query_example(query: str) -> dict:\n",
    "    cfg = Config()\n",
    "    with sql.connect(\n",
    "        server_hostname=cfg.host,\n",
    "        http_path=f\"/sql/1.0/warehouses/{DATABRICKS_WAREHOUSE_ID}\",\n",
    "        credentials_provider=lambda: cfg.authenticate\n",
    "    ) as connection:\n",
    "        with connection.cursor() as cursor:\n",
    "            cursor.execute(query)\n",
    "            df = cursor.fetchall_arrow().to_pandas()\n",
    "            if len(df) > 1000:\n",
    "                raise ValueError(\"Query result exceeds 1000 rows. Please refine your query.\")\n",
    "            return df.to_dict('index')\n",
    "        \n",
    "tool_output = sql_query_example(\"SELECT COUNT(*) FROM samples.tpch.customer\")\n",
    "\n",
    "display(Markdown(\"### Tool real output:\"))\n",
    "display_json(tool_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b98e61",
   "metadata": {},
   "source": [
    "# 5. Langchain tools\n",
    "\n",
    "Here we will use the real function. Using `tool` decorator to convert our python function to langchain `StructuredTool`. It will help also with json structure creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d21371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```json\n",
       "{\n",
       "  \"description\": \"Executing given SQL query and returning a table of results\",\n",
       "  \"properties\": {\n",
       "    \"query\": {\n",
       "      \"description\": \"SQL query to be executed\",\n",
       "      \"title\": \"Query\",\n",
       "      \"type\": \"string\"\n",
       "    }\n",
       "  },\n",
       "  \"required\": [\n",
       "    \"query\"\n",
       "  ],\n",
       "  \"title\": \"sql_query\",\n",
       "  \"type\": \"object\"\n",
       "}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from langchain_core.tools import tool\n",
    "\n",
    "# Google Style function docstrings:\n",
    "\n",
    "@tool(parse_docstring=True)\n",
    "def sql_query(query: str) -> str:\n",
    "    \"\"\"Executing given SQL query and returning a table of results\n",
    "\n",
    "    Args:\n",
    "        query: SQL query to be executed\n",
    "    \"\"\"\n",
    "    cfg = Config(profile=\"demous\")\n",
    "    with sql.connect(\n",
    "        server_hostname=cfg.host,\n",
    "        http_path=f\"/sql/1.0/warehouses/{DATABRICKS_WAREHOUSE_ID}\",\n",
    "        credentials_provider=lambda: cfg.authenticate\n",
    "    ) as connection:\n",
    "        with connection.cursor() as cursor:\n",
    "            cursor.execute(query)\n",
    "            df = cursor.fetchall_arrow().to_pandas()\n",
    "            if len(df) > 1000:\n",
    "                raise ValueError(\"Query result exceeds 1000 rows. Please refine your query.\")\n",
    "\n",
    "            return json.dumps(df.to_dict('index'), default=str)\n",
    "\n",
    "# Langchain generates the same openai function-calling json description:\n",
    "args_schema = sql_query.args_schema.model_json_schema()\n",
    "display_json(args_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dacbc1-e033-45fa-a65d-14dc9bdb6100",
   "metadata": {},
   "source": [
    "## Invoking langchain functions\n",
    "\n",
    "We can invoke a Tool calling `invoke`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd063103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Tool real output:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```json\n",
       "{\n",
       "  \"0\": {\n",
       "    \"count(1)\": 750000\n",
       "  }\n",
       "}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tool_output = sql_query.invoke({\"query\": \"SELECT COUNT(*) FROM samples.tpch.customer\"})\n",
    "\n",
    "display(Markdown(\"### Tool real output:\"))\n",
    "display_json(json.loads(tool_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397053bd-c507-4d51-b459-04868f967e0a",
   "metadata": {},
   "source": [
    "## Adding this Tool to the model:\n",
    "\n",
    "https://python.langchain.com/v0.2/docs/concepts/#tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee6608b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### LLM raw output in JSON:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```json\n",
       "{\n",
       "  \"content\": \"\",\n",
       "  \"additional_kwargs\": {\n",
       "    \"tool_calls\": [\n",
       "      {\n",
       "        \"id\": \"call_39952a6f-fc8b-4427-896d-2cf630757889\",\n",
       "        \"type\": \"function\",\n",
       "        \"function\": {\n",
       "          \"name\": \"sql_query\",\n",
       "          \"arguments\": \"{\\\"query\\\": \\\"SELECT COUNT(*) FROM samples.tpch.customer\\\"}\"\n",
       "        }\n",
       "      }\n",
       "    ]\n",
       "  },\n",
       "  \"response_metadata\": {\n",
       "    \"id\": \"chatcmpl_11e174b9-1561-4d1a-bbb7-79de48f71105\",\n",
       "    \"object\": \"chat.completion\",\n",
       "    \"created\": 1745507488,\n",
       "    \"model\": \"meta-llama-3.3-70b-instruct-121024\",\n",
       "    \"usage\": {\n",
       "      \"prompt_tokens\": 702,\n",
       "      \"completion_tokens\": 21,\n",
       "      \"total_tokens\": 723\n",
       "    },\n",
       "    \"model_name\": \"meta-llama-3.3-70b-instruct-121024\"\n",
       "  },\n",
       "  \"type\": \"ai\",\n",
       "  \"name\": null,\n",
       "  \"id\": \"run-2b7a765b-3214-49c8-8f6c-87214575be15-0\",\n",
       "  \"example\": false,\n",
       "  \"tool_calls\": [\n",
       "    {\n",
       "      \"name\": \"sql_query\",\n",
       "      \"args\": {\n",
       "        \"query\": \"SELECT COUNT(*) FROM samples.tpch.customer\"\n",
       "      },\n",
       "      \"id\": \"call_39952a6f-fc8b-4427-896d-2cf630757889\",\n",
       "      \"type\": \"tool_call\"\n",
       "    }\n",
       "  ],\n",
       "  \"invalid_tool_calls\": [],\n",
       "  \"usage_metadata\": null\n",
       "}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from langchain_core.messages import AIMessage\n",
    "\n",
    "langchain_chat_model = ChatDatabricks(\n",
    "    endpoint=SERVING_ENDPOINT,\n",
    "    temperature=0.1,\n",
    "    max_tokens=500,\n",
    ")\n",
    "model_with_tools = langchain_chat_model.bind_tools([sql_query])\n",
    "\n",
    "tool_call_result: AIMessage = model_with_tools.invoke(messages)\n",
    "\n",
    "display(Markdown(\"### LLM raw output in JSON:\"))\n",
    "display_json(tool_call_result.model_dump())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136e0836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```json\n",
       "{\n",
       "  \"0\": {\n",
       "    \"count(1)\": 750000\n",
       "  }\n",
       "}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tool_call = tool_call_result.tool_calls[0]\n",
    "\n",
    "# In our case sql_query returns a JSON string, so we can show it as JSON back:\n",
    "tool_output = sql_query.invoke(tool_call[\"args\"])\n",
    "display_json(json.loads(tool_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2172f6-1eff-4cae-8aee-47db361f7060",
   "metadata": {},
   "source": [
    "## Generating new messages array appending Assistant and Tool responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9171eb65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### New messages array (objects):"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You are helpful assistant, please provide only reliable information based on the context.', additional_kwargs={}, response_metadata={}),\n",
      " HumanMessage(content='You are helpful assistant, please provide only reliable information based on the context.', additional_kwargs={}, response_metadata={}),\n",
      " AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_39952a6f-fc8b-4427-896d-2cf630757889', 'type': 'function', 'function': {'name': 'sql_query', 'arguments': '{\"query\": \"SELECT COUNT(*) FROM samples.tpch.customer\"}'}}]}, response_metadata={'id': 'chatcmpl_11e174b9-1561-4d1a-bbb7-79de48f71105', 'object': 'chat.completion', 'created': 1745507488, 'model': 'meta-llama-3.3-70b-instruct-121024', 'usage': {'prompt_tokens': 702, 'completion_tokens': 21, 'total_tokens': 723}, 'model_name': 'meta-llama-3.3-70b-instruct-121024'}, id='run-2b7a765b-3214-49c8-8f6c-87214575be15-0', tool_calls=[{'name': 'sql_query', 'args': {'query': 'SELECT COUNT(*) FROM samples.tpch.customer'}, 'id': 'call_39952a6f-fc8b-4427-896d-2cf630757889', 'type': 'tool_call'}]),\n",
      " ToolMessage(content='{\"0\": {\"count(1)\": 750000}}', name='sql_query', tool_call_id='call_39952a6f-fc8b-4427-896d-2cf630757889')]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### New messages array (JSON):"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```json\n",
       "[\n",
       "  {\n",
       "    \"content\": \"You are helpful assistant, please provide only reliable information based on the context.\",\n",
       "    \"additional_kwargs\": {},\n",
       "    \"response_metadata\": {},\n",
       "    \"type\": \"system\",\n",
       "    \"name\": null,\n",
       "    \"id\": null\n",
       "  },\n",
       "  {\n",
       "    \"content\": \"You are helpful assistant, please provide only reliable information based on the context.\",\n",
       "    \"additional_kwargs\": {},\n",
       "    \"response_metadata\": {},\n",
       "    \"type\": \"human\",\n",
       "    \"name\": null,\n",
       "    \"id\": null,\n",
       "    \"example\": false\n",
       "  },\n",
       "  {\n",
       "    \"content\": \"\",\n",
       "    \"additional_kwargs\": {\n",
       "      \"tool_calls\": [\n",
       "        {\n",
       "          \"id\": \"call_39952a6f-fc8b-4427-896d-2cf630757889\",\n",
       "          \"type\": \"function\",\n",
       "          \"function\": {\n",
       "            \"name\": \"sql_query\",\n",
       "            \"arguments\": \"{\\\"query\\\": \\\"SELECT COUNT(*) FROM samples.tpch.customer\\\"}\"\n",
       "          }\n",
       "        }\n",
       "      ]\n",
       "    },\n",
       "    \"response_metadata\": {\n",
       "      \"id\": \"chatcmpl_11e174b9-1561-4d1a-bbb7-79de48f71105\",\n",
       "      \"object\": \"chat.completion\",\n",
       "      \"created\": 1745507488,\n",
       "      \"model\": \"meta-llama-3.3-70b-instruct-121024\",\n",
       "      \"usage\": {\n",
       "        \"prompt_tokens\": 702,\n",
       "        \"completion_tokens\": 21,\n",
       "        \"total_tokens\": 723\n",
       "      },\n",
       "      \"model_name\": \"meta-llama-3.3-70b-instruct-121024\"\n",
       "    },\n",
       "    \"type\": \"ai\",\n",
       "    \"name\": null,\n",
       "    \"id\": \"run-2b7a765b-3214-49c8-8f6c-87214575be15-0\",\n",
       "    \"example\": false,\n",
       "    \"tool_calls\": [\n",
       "      {\n",
       "        \"name\": \"sql_query\",\n",
       "        \"args\": {\n",
       "          \"query\": \"SELECT COUNT(*) FROM samples.tpch.customer\"\n",
       "        },\n",
       "        \"id\": \"call_39952a6f-fc8b-4427-896d-2cf630757889\",\n",
       "        \"type\": \"tool_call\"\n",
       "      }\n",
       "    ],\n",
       "    \"invalid_tool_calls\": [],\n",
       "    \"usage_metadata\": null\n",
       "  },\n",
       "  {\n",
       "    \"content\": \"{\\\"0\\\": {\\\"count(1)\\\": 750000}}\",\n",
       "    \"additional_kwargs\": {},\n",
       "    \"response_metadata\": {},\n",
       "    \"type\": \"tool\",\n",
       "    \"name\": \"sql_query\",\n",
       "    \"id\": null,\n",
       "    \"tool_call_id\": \"call_39952a6f-fc8b-4427-896d-2cf630757889\",\n",
       "    \"artifact\": null,\n",
       "    \"status\": \"success\"\n",
       "  }\n",
       "]\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from langchain_core.messages import SystemMessage, HumanMessage, ToolMessage, BaseMessage\n",
    "\n",
    "new_messages: list[BaseMessage] = []\n",
    "\n",
    "new_messages.append(SystemMessage(content=messages[0][\"content\"]))\n",
    "new_messages.append(HumanMessage(content=messages[1][\"content\"]))\n",
    "new_messages.append(tool_call_result)\n",
    "new_messages.append(ToolMessage(content=tool_output, tool_call_id=tool_call[\"id\"], name=tool_call[\"name\"]))\n",
    "\n",
    "display(Markdown(\"### New messages array (objects):\"))\n",
    "pprint(new_messages)\n",
    "display(Markdown(\"### New messages array (JSON):\"))\n",
    "display_json([x.model_dump() for x in new_messages])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d52903b-4db7-4c20-a3e9-e52cf8806f52",
   "metadata": {},
   "source": [
    "**Note**: You can use objects `ToolMessage`, `AIMessage`, etc. directly as elements of the `messages` array as well (without model_dump usage). But I have first two in dict format, and it is bad practice of mixing `dict` and langchain classes in the `messages` array. Langchain can work with both though even in mixed.\n",
    "\n",
    "Now, let's execute LLM with new messages array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72e355f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### LLM output in Markdown:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "I'm happy to help, but it seems like there was a misunderstanding. The conversation just started, and I didn't receive any context or question to provide reliable information about. Could you please provide more context or ask a question so I can assist you better?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = langchain_chat_model.invoke(new_messages)\n",
    "\n",
    "display(Markdown(\"### LLM output in Markdown:\"))\n",
    "display(Markdown(result.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bb77d2",
   "metadata": {},
   "source": [
    "# 6. Databricks UC functions as tools\n",
    "\n",
    "You can create UDF in Databricks and use them as Tools. Databricks already has one: `system.ai.python_exec`, it can execute any python code and return the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add2aefd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```json\n",
       "{\n",
       "  \"additionalProperties\": false,\n",
       "  \"properties\": {\n",
       "    \"code\": {\n",
       "      \"anyOf\": [\n",
       "        {\n",
       "          \"type\": \"string\"\n",
       "        },\n",
       "        {\n",
       "          \"type\": \"null\"\n",
       "        }\n",
       "      ],\n",
       "      \"default\": null,\n",
       "      \"description\": \"Python code to execute. Ensure that all variables are initialized within the code, and import any necessary standard libraries. The code must print the final result to stdout. Do not attempt to access files or external systems.\",\n",
       "      \"title\": \"Code\"\n",
       "    }\n",
       "  },\n",
       "  \"title\": \"system__ai__python_exec__params\",\n",
       "  \"type\": \"object\"\n",
       "}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from databricks_langchain import UCFunctionToolkit\n",
    "# from unitycatalog.ai.core.databricks import DatabricksFunctionClient\n",
    "\n",
    "uc_client = DatabricksFunctionClient(client=client)\n",
    "uc_toolkit = UCFunctionToolkit(function_names=[\"system.ai.python_exec\"], client=uc_client)\n",
    "\n",
    "python_exec_tool = uc_toolkit.tools_dict[\"system.ai.python_exec\"]\n",
    "\n",
    "display_json(python_exec_tool.args_schema.model_json_schema())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125b65be-b4a5-457a-b483-ab257263588a",
   "metadata": {},
   "source": [
    "**Sample**:\n",
    "\n",
    "Let's call it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8146f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```json\n",
       "{\n",
       "  \"format\": \"SCALAR\",\n",
       "  \"value\": \"15.0\\n\"\n",
       "}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tool_result = python_exec_tool.invoke({\"code\": \"\"\"\n",
    "import math\n",
    "a = 200 + 25\n",
    "print(math.sqrt(a))\n",
    "\"\"\"})\n",
    "display_json(json.loads(tool_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03071484-b5fd-48cd-a816-9c84ff6924a9",
   "metadata": {},
   "source": [
    "## Adding UC tool for LLM example:\n",
    "\n",
    "Now let's say to our LLM that we have two tools avaiable: `python_exec_tool` and `sql_query` tool. And let LLM decide which tool to call based on th user's request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9521199",
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain_chat_model = ChatDatabricks(\n",
    "    endpoint=SERVING_ENDPOINT,\n",
    "    temperature=0.1,\n",
    "    max_tokens=500,\n",
    ")\n",
    "model_with_tools = langchain_chat_model.bind_tools([python_exec_tool, sql_query])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4008566d",
   "metadata": {},
   "source": [
    "### Let's ask LLM to execute something using PYTHON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aac5dbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### LLM raw output:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```json\n",
       "{\n",
       "  \"content\": \"\",\n",
       "  \"additional_kwargs\": {\n",
       "    \"tool_calls\": [\n",
       "      {\n",
       "        \"id\": \"call_dae46b4b-be9f-4672-b95d-f4a7cb2954ea\",\n",
       "        \"type\": \"function\",\n",
       "        \"function\": {\n",
       "          \"name\": \"system__ai__python_exec\",\n",
       "          \"arguments\": \"{\\\"code\\\": \\\"import math\\\\nprint(math.pi / 2)\\\"}\"\n",
       "        }\n",
       "      }\n",
       "    ]\n",
       "  },\n",
       "  \"response_metadata\": {\n",
       "    \"id\": \"chatcmpl_b5507d7c-8b9e-4fd8-8230-578776091c6f\",\n",
       "    \"object\": \"chat.completion\",\n",
       "    \"created\": 1745507519,\n",
       "    \"model\": \"meta-llama-3.3-70b-instruct-121024\",\n",
       "    \"usage\": {\n",
       "      \"prompt_tokens\": 875,\n",
       "      \"completion_tokens\": 26,\n",
       "      \"total_tokens\": 901\n",
       "    },\n",
       "    \"model_name\": \"meta-llama-3.3-70b-instruct-121024\"\n",
       "  },\n",
       "  \"type\": \"ai\",\n",
       "  \"name\": null,\n",
       "  \"id\": \"run-87815d1e-619a-4bb0-b689-8b673f3cf6ca-0\",\n",
       "  \"example\": false,\n",
       "  \"tool_calls\": [\n",
       "    {\n",
       "      \"name\": \"system__ai__python_exec\",\n",
       "      \"args\": {\n",
       "        \"code\": \"import math\\nprint(math.pi / 2)\"\n",
       "      },\n",
       "      \"id\": \"call_dae46b4b-be9f-4672-b95d-f4a7cb2954ea\",\n",
       "      \"type\": \"tool_call\"\n",
       "    }\n",
       "  ],\n",
       "  \"invalid_tool_calls\": [],\n",
       "  \"usage_metadata\": null\n",
       "}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tool_call_request = model_with_tools.invoke([\n",
    "    {\"role\": \"user\", \"content\": \"using python calculate the pi/2\"}\n",
    "])\n",
    "\n",
    "tool_call = tool_call_request.tool_calls[0]\n",
    "\n",
    "display(Markdown(\"### LLM raw output:\"))\n",
    "display_json(vars(tool_call_request))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2539bfd-3955-4958-8a7a-245c40367b28",
   "metadata": {},
   "source": [
    "**Output**:\n",
    "\n",
    "In the output LLM asks us to execute function `system.ai.python_exec` (which we named as system__ai__python_exec for LLM) and pass the parameter `{'code': 'import math; print(math.pi / 2)'}` to answer user's question.\n",
    "\n",
    "Our program logic could look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502d5c44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### PYTHON function executed"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```json\n",
       "{\n",
       "  \"format\": \"SCALAR\",\n",
       "  \"value\": \"1.5707963267948966\\n\"\n",
       "}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if tool_call[\"name\"] == \"system__ai__python_exec\":\n",
    "    display(Markdown(\"### PYTHON function executed\"))\n",
    "    func = python_exec_tool\n",
    "elif tool_call[\"name\"] == \"sql_query\":\n",
    "    display(Markdown(\"### SQL function executed\"))\n",
    "    func = sql_query\n",
    "\n",
    "tool_output = func.invoke(tool_call[\"args\"])\n",
    "display_json(json.loads(tool_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf8fc25",
   "metadata": {},
   "source": [
    "### Let's ask the SAME LLM to execute the same as previous, but using SQL now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9598639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### LLM raw output:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```json\n",
       "{\n",
       "  \"content\": \"\",\n",
       "  \"additional_kwargs\": {\n",
       "    \"tool_calls\": [\n",
       "      {\n",
       "        \"id\": \"call_0aa60cd7-9787-4f98-9e6c-37d8fbfbfc64\",\n",
       "        \"type\": \"function\",\n",
       "        \"function\": {\n",
       "          \"name\": \"sql_query\",\n",
       "          \"arguments\": \"{\\\"query\\\": \\\"SELECT 4 * ATAN(1)\\\"}\"\n",
       "        }\n",
       "      }\n",
       "    ]\n",
       "  },\n",
       "  \"response_metadata\": {\n",
       "    \"id\": \"chatcmpl_2c2193a5-0ded-4feb-812b-f0f2d1008ad3\",\n",
       "    \"object\": \"chat.completion\",\n",
       "    \"created\": 1745507522,\n",
       "    \"model\": \"meta-llama-3.3-70b-instruct-121024\",\n",
       "    \"usage\": {\n",
       "      \"prompt_tokens\": 875,\n",
       "      \"completion_tokens\": 21,\n",
       "      \"total_tokens\": 896\n",
       "    },\n",
       "    \"model_name\": \"meta-llama-3.3-70b-instruct-121024\"\n",
       "  },\n",
       "  \"type\": \"ai\",\n",
       "  \"name\": null,\n",
       "  \"id\": \"run-60912fcd-dc87-475a-849f-2ab0cee702c9-0\",\n",
       "  \"example\": false,\n",
       "  \"tool_calls\": [\n",
       "    {\n",
       "      \"name\": \"sql_query\",\n",
       "      \"args\": {\n",
       "        \"query\": \"SELECT 4 * ATAN(1)\"\n",
       "      },\n",
       "      \"id\": \"call_0aa60cd7-9787-4f98-9e6c-37d8fbfbfc64\",\n",
       "      \"type\": \"tool_call\"\n",
       "    }\n",
       "  ],\n",
       "  \"invalid_tool_calls\": [],\n",
       "  \"usage_metadata\": null\n",
       "}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tool_call_request = model_with_tools.invoke([\n",
    "    {\"role\": \"user\", \"content\": \"using SQL calculate the pi/2\"}\n",
    "])\n",
    "\n",
    "tool_call = tool_call_request.tool_calls[0]\n",
    "\n",
    "display(Markdown(\"### LLM raw output:\"))\n",
    "display_json(vars(tool_call_request))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96490a71",
   "metadata": {},
   "source": [
    "**Output**:\n",
    "\n",
    "In the output LLM asks us to execute function `sql_query` and pass the parameter `query` with SELECT statement to answer user's question.\n",
    "\n",
    "Our program logic is the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35bd548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### SQL function executed"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```json\n",
       "{\n",
       "  \"0\": {\n",
       "    \"(4 * ATAN(1))\": 3.141592653589793\n",
       "  }\n",
       "}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if tool_call[\"name\"] == \"system__ai__python_exec\":\n",
    "    display(Markdown(\"### PYTHON function executed\"))\n",
    "    func = python_exec_tool\n",
    "elif tool_call[\"name\"] == \"sql_query\":\n",
    "    display(Markdown(\"### SQL function executed\"))\n",
    "    func = sql_query\n",
    "\n",
    "tool_output = func.invoke(tool_call[\"args\"])\n",
    "display_json(json.loads(tool_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329d7e25",
   "metadata": {},
   "source": [
    "# Performing SQL queries using Databricks and LangChain agents\n",
    "\n",
    "- https://python.langchain.com/v0.2/docs/tutorials/sql_qa/\n",
    "- Obsolete: https://python.langchain.com/v0.2/api_reference/langchain/chains/langchain.chains.sql_database.query.create_sql_query_chain.html\n",
    "- https://docs.unitycatalog.io/ai/integrations/langchain/\n",
    "\n",
    "Before, we manually executed functions based on the LLM request. Langchain has helpers to do it automatically.\n",
    "\n",
    "Agent will ask LLM to generate SQL query and will trigger our functions with provided arguments on behalf of us:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187f5041-f4ff-4942-8c89-940cb371d201",
   "metadata": {},
   "source": [
    "## Adding Callbacks for transparency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026ae6d3-afd5-453d-a4fe-fb708270bc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.callbacks import StdOutCallbackHandler\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "\n",
    "\n",
    "# Not important for demonstration, but makes logging more readable:\n",
    "class CustomLoggingCallback(StdOutCallbackHandler):\n",
    "\n",
    "    def on_tool_start(self, serialized, input_str, **kwargs):\n",
    "        print(f\"  Tool called: {serialized['name']} with input: {input_str}\")\n",
    "\n",
    "    def on_tool_end(self, output, **kwargs):\n",
    "        print(f\" Tool output: {output=}\")\n",
    "\n",
    "    def on_llm_start(self, serialized, prompts, **kwargs):\n",
    "        print(f\" LLM Prompt: {prompts=}\")\n",
    "\n",
    "    def on_llm_end(self, response, **kwargs):\n",
    "        print(f\" LLM Response: {response=}\")\n",
    "\n",
    "    def on_chain_start(self, serialized, inputs, run_id, **kwargs):\n",
    "        print(f\" Chain start: {inputs=}\")\n",
    "\n",
    "    def on_chain_end(self, outputs, **kwargs):\n",
    "        print(f\" Chain end: {outputs=}\")\n",
    "\n",
    "    def on_agent_action(self, action, **kwargs):\n",
    "        print(f\"  Agent action: {action=} with {kwargs=}\")\n",
    "\n",
    "\n",
    "langchain_chat_model = ChatDatabricks(\n",
    "    endpoint=\"databricks-meta-llama-3-1-405b-instruct\",\n",
    "    temperature=0.1,\n",
    "    max_tokens=500,\n",
    "    callbacks=[CustomLoggingCallback()]\n",
    ")\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Always use the available tools for calculations. \"\n",
    "            \"Ensure Python code is formatted with line breaks.\"\n",
    "        ),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "agent = create_tool_calling_agent(langchain_chat_model, [sql_query, python_exec_tool], prompt)\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=[sql_query, python_exec_tool],\n",
    "    verbose=True,\n",
    "    return_intermediate_steps=True,\n",
    "    callbacks=[CustomLoggingCallback()],\n",
    "    max_iterations=8,\n",
    "    handle_parsing_errors=True,\n",
    "    handle_tool_errors=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d6a0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Chain start: inputs={'input': '\\nCount the amount of records in table samples.nyctaxi.trips where trip distance is less than 5.\\nFind a square root of the result using python_exec tool. \\n'}\n",
      " LLM Prompt: prompts=['System: You are a helpful assistant. Always use the available tools for calculations. Ensure Python code is formatted with line breaks.\\nHuman: \\nCount the amount of records in table samples.nyctaxi.trips where trip distance is less than 5.\\nFind a square root of the result using python_exec tool. \\n']\n",
      " LLM Response: response=LLMResult(generations=[[ChatGenerationChunk(generation_info={'finish_reason': 'tool_calls'}, message=AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'id': 'call_126170bc-d64a-4f24-8238-9418c8819bd9', 'index': 0, 'type': 'function', 'function': {'name': 'sql_query', 'arguments': '{\"query\": \"SELECT COUNT(*) FROM samples.nyctaxi.trips WHERE trip_distance < 5\"}'}}]}, response_metadata={'finish_reason': 'tool_calls'}, id='run-df8e6dd7-32e7-487f-9b97-721157ff0e65', tool_calls=[{'name': 'sql_query', 'args': {'query': 'SELECT COUNT(*) FROM samples.nyctaxi.trips WHERE trip_distance < 5'}, 'id': 'call_126170bc-d64a-4f24-8238-9418c8819bd9', 'type': 'tool_call'}], tool_call_chunks=[{'name': 'sql_query', 'args': '{\"query\": \"SELECT COUNT(*) FROM samples.nyctaxi.trips WHERE trip_distance < 5\"}', 'id': 'call_126170bc-d64a-4f24-8238-9418c8819bd9', 'index': 0, 'type': 'tool_call_chunk'}]))]], llm_output=None, run=None, type='LLMResult')\n",
      "  Agent action: action=ToolAgentAction(tool='sql_query', tool_input={'query': 'SELECT COUNT(*) FROM samples.nyctaxi.trips WHERE trip_distance < 5'}, log=\"\\nInvoking: `sql_query` with `{'query': 'SELECT COUNT(*) FROM samples.nyctaxi.trips WHERE trip_distance < 5'}`\\n\\n\\n\", message_log=[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'id': 'call_126170bc-d64a-4f24-8238-9418c8819bd9', 'index': 0, 'type': 'function', 'function': {'name': 'sql_query', 'arguments': '{\"query\": \"SELECT COUNT(*) FROM samples.nyctaxi.trips WHERE trip_distance < 5\"}'}}]}, response_metadata={'finish_reason': 'tool_calls'}, id='run-df8e6dd7-32e7-487f-9b97-721157ff0e65', tool_calls=[{'name': 'sql_query', 'args': {'query': 'SELECT COUNT(*) FROM samples.nyctaxi.trips WHERE trip_distance < 5'}, 'id': 'call_126170bc-d64a-4f24-8238-9418c8819bd9', 'type': 'tool_call'}], tool_call_chunks=[{'name': 'sql_query', 'args': '{\"query\": \"SELECT COUNT(*) FROM samples.nyctaxi.trips WHERE trip_distance < 5\"}', 'id': 'call_126170bc-d64a-4f24-8238-9418c8819bd9', 'index': 0, 'type': 'tool_call_chunk'}])], tool_call_id='call_126170bc-d64a-4f24-8238-9418c8819bd9') with kwargs={'run_id': UUID('dcc94fbd-ad6a-4db1-b909-43baef1bede8'), 'parent_run_id': None, 'tags': [], 'color': 'green'}\n",
      "\u001b[36;1m\u001b[1;3m{\"0\": {\"count(1)\": 18929}}\u001b[0m LLM Prompt: prompts=['System: You are a helpful assistant. Always use the available tools for calculations. Ensure Python code is formatted with line breaks.\\nHuman: \\nCount the amount of records in table samples.nyctaxi.trips where trip distance is less than 5.\\nFind a square root of the result using python_exec tool. \\n\\nAI: \\nTool: {\"0\": {\"count(1)\": 18929}}']\n",
      " LLM Response: response=LLMResult(generations=[[ChatGenerationChunk(generation_info={'finish_reason': 'tool_calls'}, message=AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'id': 'call_8b1bc7ad-7cad-4d39-bb4f-c9996778b101', 'index': 0, 'type': 'function', 'function': {'name': 'system__ai__python_exec', 'arguments': '{\"code\": \"import math\\\\\\\\nresult = 18929\\\\\\\\nprint(math.sqrt(result))\"}'}}]}, response_metadata={'finish_reason': 'tool_calls'}, id='run-f61e14c4-fcdc-4522-9282-fa8c6d5dc896', tool_calls=[{'name': 'system__ai__python_exec', 'args': {'code': 'import math\\\\nresult = 18929\\\\nprint(math.sqrt(result))'}, 'id': 'call_8b1bc7ad-7cad-4d39-bb4f-c9996778b101', 'type': 'tool_call'}], tool_call_chunks=[{'name': 'system__ai__python_exec', 'args': '{\"code\": \"import math\\\\\\\\nresult = 18929\\\\\\\\nprint(math.sqrt(result))\"}', 'id': 'call_8b1bc7ad-7cad-4d39-bb4f-c9996778b101', 'index': 0, 'type': 'tool_call_chunk'}]))]], llm_output=None, run=None, type='LLMResult')\n",
      "  Agent action: action=ToolAgentAction(tool='system__ai__python_exec', tool_input={'code': 'import math\\\\nresult = 18929\\\\nprint(math.sqrt(result))'}, log=\"\\nInvoking: `system__ai__python_exec` with `{'code': 'import math\\\\\\\\nresult = 18929\\\\\\\\nprint(math.sqrt(result))'}`\\n\\n\\n\", message_log=[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'id': 'call_8b1bc7ad-7cad-4d39-bb4f-c9996778b101', 'index': 0, 'type': 'function', 'function': {'name': 'system__ai__python_exec', 'arguments': '{\"code\": \"import math\\\\\\\\nresult = 18929\\\\\\\\nprint(math.sqrt(result))\"}'}}]}, response_metadata={'finish_reason': 'tool_calls'}, id='run-f61e14c4-fcdc-4522-9282-fa8c6d5dc896', tool_calls=[{'name': 'system__ai__python_exec', 'args': {'code': 'import math\\\\nresult = 18929\\\\nprint(math.sqrt(result))'}, 'id': 'call_8b1bc7ad-7cad-4d39-bb4f-c9996778b101', 'type': 'tool_call'}], tool_call_chunks=[{'name': 'system__ai__python_exec', 'args': '{\"code\": \"import math\\\\\\\\nresult = 18929\\\\\\\\nprint(math.sqrt(result))\"}', 'id': 'call_8b1bc7ad-7cad-4d39-bb4f-c9996778b101', 'index': 0, 'type': 'tool_call_chunk'}])], tool_call_id='call_8b1bc7ad-7cad-4d39-bb4f-c9996778b101') with kwargs={'run_id': UUID('dcc94fbd-ad6a-4db1-b909-43baef1bede8'), 'parent_run_id': None, 'tags': [], 'color': 'green'}\n",
      "\u001b[33;1m\u001b[1;3m{\"format\": \"SCALAR\", \"value\": \"137.58270240113762\\n\"}\u001b[0m LLM Prompt: prompts=['System: You are a helpful assistant. Always use the available tools for calculations. Ensure Python code is formatted with line breaks.\\nHuman: \\nCount the amount of records in table samples.nyctaxi.trips where trip distance is less than 5.\\nFind a square root of the result using python_exec tool. \\n\\nAI: \\nTool: {\"0\": {\"count(1)\": 18929}}\\nAI: \\nTool: {\"format\": \"SCALAR\", \"value\": \"137.58270240113762\\\\n\"}']\n",
      " LLM Response: response=LLMResult(generations=[[ChatGenerationChunk(text='The square root of the count of records in the table samples.nyctaxi.trips where trip distance is less than 5 is 137.58270240113762.', generation_info={'finish_reason': 'stop'}, message=AIMessageChunk(content='The square root of the count of records in the table samples.nyctaxi.trips where trip distance is less than 5 is 137.58270240113762.', additional_kwargs={}, response_metadata={'finish_reason': 'stop'}, id='run-05f63ca2-507f-4e89-bde5-a1f1c3b59335'))]], llm_output=None, run=None, type='LLMResult')\n",
      "\u001b[32;1m\u001b[1;3mThe square root of the count of records in the table samples.nyctaxi.trips where trip distance is less than 5 is 137.58270240113762.\u001b[0m\n",
      " Chain end: outputs={'output': 'The square root of the count of records in the table samples.nyctaxi.trips where trip distance is less than 5 is 137.58270240113762.', 'intermediate_steps': [(ToolAgentAction(tool='sql_query', tool_input={'query': 'SELECT COUNT(*) FROM samples.nyctaxi.trips WHERE trip_distance < 5'}, log=\"\\nInvoking: `sql_query` with `{'query': 'SELECT COUNT(*) FROM samples.nyctaxi.trips WHERE trip_distance < 5'}`\\n\\n\\n\", message_log=[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'id': 'call_126170bc-d64a-4f24-8238-9418c8819bd9', 'index': 0, 'type': 'function', 'function': {'name': 'sql_query', 'arguments': '{\"query\": \"SELECT COUNT(*) FROM samples.nyctaxi.trips WHERE trip_distance < 5\"}'}}]}, response_metadata={'finish_reason': 'tool_calls'}, id='run-df8e6dd7-32e7-487f-9b97-721157ff0e65', tool_calls=[{'name': 'sql_query', 'args': {'query': 'SELECT COUNT(*) FROM samples.nyctaxi.trips WHERE trip_distance < 5'}, 'id': 'call_126170bc-d64a-4f24-8238-9418c8819bd9', 'type': 'tool_call'}], tool_call_chunks=[{'name': 'sql_query', 'args': '{\"query\": \"SELECT COUNT(*) FROM samples.nyctaxi.trips WHERE trip_distance < 5\"}', 'id': 'call_126170bc-d64a-4f24-8238-9418c8819bd9', 'index': 0, 'type': 'tool_call_chunk'}])], tool_call_id='call_126170bc-d64a-4f24-8238-9418c8819bd9'), '{\"0\": {\"count(1)\": 18929}}'), (ToolAgentAction(tool='system__ai__python_exec', tool_input={'code': 'import math\\\\nresult = 18929\\\\nprint(math.sqrt(result))'}, log=\"\\nInvoking: `system__ai__python_exec` with `{'code': 'import math\\\\\\\\nresult = 18929\\\\\\\\nprint(math.sqrt(result))'}`\\n\\n\\n\", message_log=[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'id': 'call_8b1bc7ad-7cad-4d39-bb4f-c9996778b101', 'index': 0, 'type': 'function', 'function': {'name': 'system__ai__python_exec', 'arguments': '{\"code\": \"import math\\\\\\\\nresult = 18929\\\\\\\\nprint(math.sqrt(result))\"}'}}]}, response_metadata={'finish_reason': 'tool_calls'}, id='run-f61e14c4-fcdc-4522-9282-fa8c6d5dc896', tool_calls=[{'name': 'system__ai__python_exec', 'args': {'code': 'import math\\\\nresult = 18929\\\\nprint(math.sqrt(result))'}, 'id': 'call_8b1bc7ad-7cad-4d39-bb4f-c9996778b101', 'type': 'tool_call'}], tool_call_chunks=[{'name': 'system__ai__python_exec', 'args': '{\"code\": \"import math\\\\\\\\nresult = 18929\\\\\\\\nprint(math.sqrt(result))\"}', 'id': 'call_8b1bc7ad-7cad-4d39-bb4f-c9996778b101', 'index': 0, 'type': 'tool_call_chunk'}])], tool_call_id='call_8b1bc7ad-7cad-4d39-bb4f-c9996778b101'), '{\"format\": \"SCALAR\", \"value\": \"137.58270240113762\\\\n\"}')]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': '\\nCount the amount of records in table samples.nyctaxi.trips where trip distance is less than 5.\\nFind a square root of the result using python_exec tool. \\n',\n",
       " 'output': 'The square root of the count of records in the table samples.nyctaxi.trips where trip distance is less than 5 is 137.58270240113762.',\n",
       " 'intermediate_steps': [(ToolAgentAction(tool='sql_query', tool_input={'query': 'SELECT COUNT(*) FROM samples.nyctaxi.trips WHERE trip_distance < 5'}, log=\"\\nInvoking: `sql_query` with `{'query': 'SELECT COUNT(*) FROM samples.nyctaxi.trips WHERE trip_distance < 5'}`\\n\\n\\n\", message_log=[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'id': 'call_126170bc-d64a-4f24-8238-9418c8819bd9', 'index': 0, 'type': 'function', 'function': {'name': 'sql_query', 'arguments': '{\"query\": \"SELECT COUNT(*) FROM samples.nyctaxi.trips WHERE trip_distance < 5\"}'}}]}, response_metadata={'finish_reason': 'tool_calls'}, id='run-df8e6dd7-32e7-487f-9b97-721157ff0e65', tool_calls=[{'name': 'sql_query', 'args': {'query': 'SELECT COUNT(*) FROM samples.nyctaxi.trips WHERE trip_distance < 5'}, 'id': 'call_126170bc-d64a-4f24-8238-9418c8819bd9', 'type': 'tool_call'}], tool_call_chunks=[{'name': 'sql_query', 'args': '{\"query\": \"SELECT COUNT(*) FROM samples.nyctaxi.trips WHERE trip_distance < 5\"}', 'id': 'call_126170bc-d64a-4f24-8238-9418c8819bd9', 'index': 0, 'type': 'tool_call_chunk'}])], tool_call_id='call_126170bc-d64a-4f24-8238-9418c8819bd9'),\n",
       "   '{\"0\": {\"count(1)\": 18929}}'),\n",
       "  (ToolAgentAction(tool='system__ai__python_exec', tool_input={'code': 'import math\\\\nresult = 18929\\\\nprint(math.sqrt(result))'}, log=\"\\nInvoking: `system__ai__python_exec` with `{'code': 'import math\\\\\\\\nresult = 18929\\\\\\\\nprint(math.sqrt(result))'}`\\n\\n\\n\", message_log=[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'id': 'call_8b1bc7ad-7cad-4d39-bb4f-c9996778b101', 'index': 0, 'type': 'function', 'function': {'name': 'system__ai__python_exec', 'arguments': '{\"code\": \"import math\\\\\\\\nresult = 18929\\\\\\\\nprint(math.sqrt(result))\"}'}}]}, response_metadata={'finish_reason': 'tool_calls'}, id='run-f61e14c4-fcdc-4522-9282-fa8c6d5dc896', tool_calls=[{'name': 'system__ai__python_exec', 'args': {'code': 'import math\\\\nresult = 18929\\\\nprint(math.sqrt(result))'}, 'id': 'call_8b1bc7ad-7cad-4d39-bb4f-c9996778b101', 'type': 'tool_call'}], tool_call_chunks=[{'name': 'system__ai__python_exec', 'args': '{\"code\": \"import math\\\\\\\\nresult = 18929\\\\\\\\nprint(math.sqrt(result))\"}', 'id': 'call_8b1bc7ad-7cad-4d39-bb4f-c9996778b101', 'index': 0, 'type': 'tool_call_chunk'}])], tool_call_id='call_8b1bc7ad-7cad-4d39-bb4f-c9996778b101'),\n",
       "   '{\"format\": \"SCALAR\", \"value\": \"137.58270240113762\\\\n\"}')]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = agent_executor.invoke({\n",
    "    \"input\": \n",
    "\"\"\"\n",
    "Count the amount of records in table samples.nyctaxi.trips where trip distance is less than 5.\n",
    "Find a square root of the result using python_exec tool. \n",
    "\"\"\"\n",
    "})\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beab3a20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Agent Markdown output:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The square root of the count of records in the table samples.nyctaxi.trips where trip distance is less than 5 is 137.58270240113762."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(\"### Agent Markdown output:\"))\n",
    "display(Markdown(res[\"output\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f91aab7-0dcc-418e-a017-fa46d293843a",
   "metadata": {},
   "source": [
    "# Limitations / Bugs\n",
    "\n",
    "Many LLM models cannot return content and function at the same time, but sometimes they want (they want to comment or explain their actions before function call). For example LLAMA in this case returns content like this one: \n",
    "\n",
    "**LLM's content field:**\n",
    "To calculate this we need that... Now, we will use this value of B in our SQL query: <function=sql_query>{\"query\": \"SELECT COUNT(*) as count FROM samples.nyctaxi.trips WHERE trip_distance < 0.67\"}</function>\n",
    "\n",
    "So, actually it adds a tag `function` inside the content and believes that asks for a function call. OpenAI models, for instance, can provide both content and function, so they do this properly. \"Standard\" modules from langchain cannot process such behavior (which is quite common).\n",
    "\n",
    "**Example:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68108798-dadc-4b4f-b679-4bcc00583309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LLM Prompt: prompts=['System: You are a helpful assistant. Always use tools results and do not rely on your knowledge.\\nHuman: Using python execute 12321342*23423423/22234. Discuss your decision before the function call.']\n",
      " LLM Response: response=LLMResult(generations=[[ChatGeneration(text='To answer the user\\'s question, we need to execute a mathematical expression in Python. The expression is 12321342*23423423/22234. Since this involves executing Python code, the \"exec_code\" function is relevant here.\\n\\n<function=exec_code>{\"code\": \"print(12321342*23423423/22234)\"}</function>', generation_info={}, message=AIMessage(content='To answer the user\\'s question, we need to execute a mathematical expression in Python. The expression is 12321342*23423423/22234. Since this involves executing Python code, the \"exec_code\" function is relevant here.\\n\\n<function=exec_code>{\"code\": \"print(12321342*23423423/22234)\"}</function>', additional_kwargs={}, response_metadata={'id': 'chatcmpl_8fc6684a-479e-410d-8a32-7775de28289c', 'object': 'chat.completion', 'created': 1745507539, 'model': 'meta-llama-3.1-405b-instruct-081924', 'usage': {'prompt_tokens': 715, 'completion_tokens': 75, 'total_tokens': 790}, 'model_name': 'meta-llama-3.1-405b-instruct-081924'}, id='run-89305471-b2e7-4448-9091-05ff9d15ad7d-0'))]], llm_output={'id': 'chatcmpl_8fc6684a-479e-410d-8a32-7775de28289c', 'object': 'chat.completion', 'created': 1745507539, 'model': 'meta-llama-3.1-405b-instruct-081924', 'usage': {'prompt_tokens': 715, 'completion_tokens': 75, 'total_tokens': 790}, 'model_name': 'meta-llama-3.1-405b-instruct-081924'}, run=None, type='LLMResult')\n"
     ]
    }
   ],
   "source": [
    "@tool(parse_docstring=True)\n",
    "def exec_code(code: str) -> str:\n",
    "    \"\"\"Executing given python code and returning the output\n",
    "\n",
    "    Args:\n",
    "        code: python code to call\n",
    "    \"\"\"\n",
    "    # Fake result\n",
    "    return \"2310\"\n",
    "\n",
    "llm2 = langchain_chat_model.bind_tools([exec_code])\n",
    "res = llm2.invoke([\n",
    "    {\"role\":\"system\", \"content\":\"You are a helpful assistant. Always use tools results and do not rely on your knowledge.\"},\n",
    "    {\"role\":\"user\", \"content\":\"Using python execute 12321342*23423423/22234. Discuss your decision before the function call.\"}\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe83d8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```json\n",
       "{\n",
       "  \"content\": \"To answer the user's question, we need to execute a mathematical expression in Python. The expression is 12321342*23423423/22234. Since this involves executing Python code, the \\\"exec_code\\\" function is relevant here.\\n\\n<function=exec_code>{\\\"code\\\": \\\"print(12321342*23423423/22234)\\\"}</function>\",\n",
       "  \"additional_kwargs\": {},\n",
       "  \"response_metadata\": {\n",
       "    \"id\": \"chatcmpl_8fc6684a-479e-410d-8a32-7775de28289c\",\n",
       "    \"object\": \"chat.completion\",\n",
       "    \"created\": 1745507539,\n",
       "    \"model\": \"meta-llama-3.1-405b-instruct-081924\",\n",
       "    \"usage\": {\n",
       "      \"prompt_tokens\": 715,\n",
       "      \"completion_tokens\": 75,\n",
       "      \"total_tokens\": 790\n",
       "    },\n",
       "    \"model_name\": \"meta-llama-3.1-405b-instruct-081924\"\n",
       "  },\n",
       "  \"type\": \"ai\",\n",
       "  \"name\": null,\n",
       "  \"id\": \"run-89305471-b2e7-4448-9091-05ff9d15ad7d-0\",\n",
       "  \"example\": false,\n",
       "  \"tool_calls\": [],\n",
       "  \"invalid_tool_calls\": [],\n",
       "  \"usage_metadata\": null\n",
       "}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_json(vars(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e2366c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### RAW LLM content:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('To answer the user\\'s question, we need to execute a mathematical expression in Python. The expression is 12321342*23423423/22234. Since this involves executing Python code, the \"exec_code\" '\n",
      " 'function is relevant here.\\n'\n",
      " '\\n'\n",
      " '<function=exec_code>{\"code\": \"print(12321342*23423423/22234)\"}</function>')\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### What user see and should not see:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "To answer the user's question, we need to execute a mathematical expression in Python. The expression is 12321342*23423423/22234. Since this involves executing Python code, the \"exec_code\" function is relevant here.\n",
       "\n",
       "<function=exec_code>{\"code\": \"print(12321342*23423423/22234)\"}</function>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### LLM function call is empty:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'res.tool_calls=[]'\n"
     ]
    }
   ],
   "source": [
    "display(Markdown(\"### RAW LLM content:\"))\n",
    "pprint(res.content, width=200)\n",
    "display(Markdown(\"### What user see and should not see like `function` tag:\"))\n",
    "display(Markdown(res.content))\n",
    "display(Markdown(\"### LLM function call is empty (which is wrong):\"))\n",
    "pprint(f\"{res.tool_calls=}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
